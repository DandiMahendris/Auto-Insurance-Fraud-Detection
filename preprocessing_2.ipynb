{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Load Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import src.util as util\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Configuration File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = util.load_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(config_data: dict) -> pd.DataFrame:\n",
    "    # Load set of data\n",
    "    x_train = util.pickle_load(config_data[\"train_set_path\"][0])\n",
    "    y_train = util.pickle_load(config_data[\"train_set_path\"][1])\n",
    "\n",
    "    x_valid = util.pickle_load(config_data[\"valid_set_path\"][0])\n",
    "    y_valid = util.pickle_load(config_data[\"valid_set_path\"][1])\n",
    "\n",
    "    x_test = util.pickle_load(config_data[\"test_set_path\"][0])\n",
    "    y_test = util.pickle_load(config_data[\"test_set_path\"][1])\n",
    "\n",
    "    # concatenate x and y each set\n",
    "    train_set = pd.concat([x_train, y_train], axis = 1)\n",
    "    valid_set = pd.concat([x_valid, y_valid], axis = 1)\n",
    "    test_set = pd.concat([x_test, y_test], axis = 1)\n",
    "\n",
    "    # return 3 set of data\n",
    "    return train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = load_dataset(config_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Removing Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outlier(set_data):\n",
    "    set_data = set_data.copy()\n",
    "    list_of_set_data = list()\n",
    "\n",
    "    # set_data = set_data.drop(['umbrella_limit'], axis = 1)\n",
    "    # config_data_num = config_data['int32_col'].copy()\n",
    "    # config_data_num = [x for x in config_data_num if x != 'umbrella_limit']\n",
    "\n",
    "    for col in set_data[config_data['int32_col']]:\n",
    "        q1 = set_data[col].quantile(0.25)\n",
    "        q3 = set_data[col].quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "\n",
    "        set_data_cleaned = set_data[~((set_data[col] < (q1 - 1.5*iqr)) |\n",
    "                                    (set_data[col] > (q3 + 1.5*iqr)))].copy()\n",
    "        list_of_set_data.append(set_data_cleaned.copy())\n",
    "\n",
    "    set_data_cleaned = pd.concat(list_of_set_data)\n",
    "    count_duplicated_index = set_data_cleaned.index.value_counts()\n",
    "    used_index_data = count_duplicated_index[count_duplicated_index == len(config_data['int32_col'])].index\n",
    "    set_data_cleaned = set_data_cleaned.loc[used_index_data].drop_duplicates()\n",
    "\n",
    "    return set_data_cleaned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_out = remove_outlier(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Handling Missing Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Splitting data into X_train & y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitxy(set_data):\n",
    "    x_data = set_data.drop(columns = config_data['label'], axis = 1)\n",
    "    y_data = set_data[config_data['label']]\n",
    "\n",
    "    return x_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = splitxy(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Splitting data into Numerical & Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitNumCat(set_data):\n",
    "    numerical_col = config_data['int32_col']\n",
    "    categorical_col = config_data['object_predictor']\n",
    "\n",
    "    x_train_num = set_data[numerical_col]\n",
    "    x_train_cat = set_data[categorical_col]\n",
    "\n",
    "    return  x_train_num, x_train_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_num, x_train_cat = splitNumCat(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Handling numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "months_as_customer          False\n",
       "age                         False\n",
       "policy_number               False\n",
       "policy_annual_premium       False\n",
       "insured_zip                 False\n",
       "capital-gains               False\n",
       "capital-loss                False\n",
       "incident_hour_of_the_day    False\n",
       "total_claim_amount          False\n",
       "injury_claim                False\n",
       "property_claim              False\n",
       "vehicle_claim               False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_num.isna().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Handling missing value on numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform sanity check for any missing value for future data\n",
    "\n",
    "def imputerNum(data, imputer = None):\n",
    "    if imputer == None:\n",
    "        # Create imputer based on median value\n",
    "        imputer = SimpleImputer(missing_values = np.nan,\n",
    "                                strategy = \"median\")\n",
    "        imputer.fit(data)\n",
    "\n",
    "    # Transform data dengan imputer\n",
    "    # else:\n",
    "    data_imputed = pd.DataFrame(imputer.transform(data),\n",
    "                                index = data.index,\n",
    "                                columns = data.columns)\n",
    "    \n",
    "    # Convert data_imputed to int32\n",
    "    data_imputed = data_imputed.astype('int32')\n",
    "    \n",
    "    return data_imputed, imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_num_imputed, imputer_num = imputerNum(data = x_train_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Handling Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4.1 Handling missing value for Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputerCat(data, imputer = None):\n",
    "    data.umbrella_limit = data.umbrella_limit.replace('-1000000','1000000')\n",
    "\n",
    "    for col in ['collision_type','property_damage','police_report_available']:\n",
    "        data[col] = data[col].replace('?', 'UNKNOWN')\n",
    "        \n",
    "    if imputer == None:\n",
    "        # Create Imputer\n",
    "        imputer = SimpleImputer(missing_values = np.nan,\n",
    "                                strategy = 'constant',\n",
    "                                fill_value = 'UNKNOWN')\n",
    "        imputer.fit(data)\n",
    "\n",
    "    # Transform data with imputer\n",
    "    data_imputed = imputer.transform(data)\n",
    "    data_imputed = pd.DataFrame(data_imputed,\n",
    "                                index = data.index,\n",
    "                                columns = data.columns)\n",
    "\n",
    "    return data_imputed, imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_11728\\1163776876.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.umbrella_limit = data.umbrella_limit.replace('-1000000','1000000')\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_11728\\1163776876.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[col] = data[col].replace('?', 'UNKNOWN')\n"
     ]
    }
   ],
   "source": [
    "x_train_cat_imputed, imputer_cat = imputerCat(data = x_train_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2 One Hot Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
    "\n",
    "nominal = ['policy_state','policy_csl','policy_deductable','insured_sex','insured_hobbies','collision_type',\n",
    "            'authorities_contacted','incident_state','incident_city','property_damage','police_report_available',\n",
    "            'auto_make','auto_model']\n",
    "ordinal = ['incident_type','witnesses','incident_severity','auto_year','umbrella_limit','bodily_injuries',\n",
    "            'number_of_vehicles_involved']\n",
    "\n",
    "def OHEcat(data, encoder_col = None, encoder = None) -> pd.DataFrame:\n",
    "\n",
    "    data_ohe = data[nominal]\n",
    "\n",
    "    if encoder == None:\n",
    "        # Create Object\n",
    "        encoder = OneHotEncoder(handle_unknown = 'ignore',\n",
    "                                drop = 'if_binary')\n",
    "        encoder.fit(data_ohe)\n",
    "        encoder_col = encoder.get_feature_names_out(data_ohe.columns)\n",
    "    \n",
    "    \n",
    "    # Transform the data\n",
    "    data_encoded = encoder.transform(data_ohe).toarray()\n",
    "    data_encoded = pd.DataFrame(data_encoded,\n",
    "                                index = data_ohe.index,\n",
    "                                columns = encoder_col)\n",
    "    \n",
    "    # Save the object\n",
    "    util.pickle_dump(encoder, config_data[\"ohe_stasiun_path\"])\n",
    "\n",
    "    return data_encoded, encoder_col, encoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.4 Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LEcat(data, encoder = None) -> pd.DataFrame:\n",
    "\n",
    "    data_le = data[ordinal]\n",
    "\n",
    "    bodily_injuries = ['0','1','2']\n",
    "    witnesses = ['0','1','2','3']\n",
    "    umbrella_limit = ['0', '1000000', '2000000', '3000000', '4000000', '5000000', '6000000',\n",
    "                      '7000000','8000000','9000000','10000000']\n",
    "    incident_severity = ['Trivial Damage','Minor Damage','Major Damage','Total Loss']\n",
    "    incident_type = ['Parked Car','Single Vehicle Collision','Multi-vehicle Collision','Vehicle Theft']\n",
    "    auto_year = sorted(data_le.auto_year.unique())\n",
    "    number_of_vehicles_involved = ['1','2','3','4']\n",
    "\n",
    "    if encoder == None:\n",
    "        # Create object\n",
    "        encoder = OrdinalEncoder(categories=[incident_type,witnesses,incident_severity,auto_year,\n",
    "                                   umbrella_limit,bodily_injuries,number_of_vehicles_involved])\n",
    "        encoder.fit(data_le)\n",
    "\n",
    "    ## Transform the data\n",
    "    data_encoded = encoder.transform(data_le)\n",
    "    data_encoded = pd.DataFrame(data_encoded,\n",
    "                                index = data_le.index,\n",
    "                                columns = data_le.columns)\n",
    "    \n",
    "    # save the object\n",
    "    util.pickle_dump(encoder, config_data[\"le_encoder_path\"])\n",
    "\n",
    "    return data_encoded, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.5 Encoding data categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cat_ohe, encoder_ohe_col, encoder_ohe = OHEcat(data = x_train_cat_imputed)\n",
    "x_train_cat_le, encoder_le = LEcat(data = x_train_cat_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cat_concat = pd.concat([x_train_cat_ohe,x_train_cat_le], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Concatenate Numerical data & Categorical encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_concat = pd.concat([x_train_num_imputed, x_train_cat_concat], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_numcat(data_num, data_cat_ohe, data_cat_le):\n",
    "    data_cat = pd.concat([data_cat_ohe, data_cat_le], axis=1)\n",
    "    data_concat = pd.concat([data_num, data_cat], axis=1)\n",
    "\n",
    "    return data_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>months_as_customer</th>\n",
       "      <th>age</th>\n",
       "      <th>policy_number</th>\n",
       "      <th>policy_annual_premium</th>\n",
       "      <th>insured_zip</th>\n",
       "      <th>capital-gains</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>incident_hour_of_the_day</th>\n",
       "      <th>total_claim_amount</th>\n",
       "      <th>injury_claim</th>\n",
       "      <th>...</th>\n",
       "      <th>auto_model_Wrangler</th>\n",
       "      <th>auto_model_X5</th>\n",
       "      <th>auto_model_X6</th>\n",
       "      <th>incident_type</th>\n",
       "      <th>witnesses</th>\n",
       "      <th>incident_severity</th>\n",
       "      <th>auto_year</th>\n",
       "      <th>umbrella_limit</th>\n",
       "      <th>bodily_injuries</th>\n",
       "      <th>number_of_vehicles_involved</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>441</td>\n",
       "      <td>55</td>\n",
       "      <td>669501</td>\n",
       "      <td>1270</td>\n",
       "      <td>449421</td>\n",
       "      <td>24000</td>\n",
       "      <td>-50500</td>\n",
       "      <td>4</td>\n",
       "      <td>6400</td>\n",
       "      <td>640</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>275</td>\n",
       "      <td>45</td>\n",
       "      <td>403737</td>\n",
       "      <td>1447</td>\n",
       "      <td>605756</td>\n",
       "      <td>39400</td>\n",
       "      <td>-63900</td>\n",
       "      <td>8</td>\n",
       "      <td>64320</td>\n",
       "      <td>5360</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>421</td>\n",
       "      <td>56</td>\n",
       "      <td>728025</td>\n",
       "      <td>1935</td>\n",
       "      <td>470826</td>\n",
       "      <td>49500</td>\n",
       "      <td>-81100</td>\n",
       "      <td>7</td>\n",
       "      <td>92730</td>\n",
       "      <td>16860</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>424358</td>\n",
       "      <td>1282</td>\n",
       "      <td>616126</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66880</td>\n",
       "      <td>6080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>257</td>\n",
       "      <td>44</td>\n",
       "      <td>109392</td>\n",
       "      <td>1280</td>\n",
       "      <td>433981</td>\n",
       "      <td>59400</td>\n",
       "      <td>-32200</td>\n",
       "      <td>21</td>\n",
       "      <td>46980</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     months_as_customer  age  policy_number  policy_annual_premium  \\\n",
       "887                 441   55         669501                   1270   \n",
       "317                 275   45         403737                   1447   \n",
       "796                 421   56         728025                   1935   \n",
       "425                   4   34         424358                   1282   \n",
       "991                 257   44         109392                   1280   \n",
       "\n",
       "     insured_zip  capital-gains  capital-loss  incident_hour_of_the_day  \\\n",
       "887       449421          24000        -50500                         4   \n",
       "317       605756          39400        -63900                         8   \n",
       "796       470826          49500        -81100                         7   \n",
       "425       616126              0             0                         0   \n",
       "991       433981          59400        -32200                        21   \n",
       "\n",
       "     total_claim_amount  injury_claim  ...  auto_model_Wrangler  \\\n",
       "887                6400           640  ...                  0.0   \n",
       "317               64320          5360  ...                  0.0   \n",
       "796               92730         16860  ...                  0.0   \n",
       "425               66880          6080  ...                  0.0   \n",
       "991               46980             0  ...                  0.0   \n",
       "\n",
       "     auto_model_X5  auto_model_X6  incident_type  witnesses  \\\n",
       "887            0.0            0.0            0.0        0.0   \n",
       "317            0.0            0.0            2.0        1.0   \n",
       "796            0.0            0.0            1.0        3.0   \n",
       "425            0.0            0.0            2.0        0.0   \n",
       "991            0.0            0.0            1.0        1.0   \n",
       "\n",
       "     incident_severity  auto_year  umbrella_limit  bodily_injuries  \\\n",
       "887                1.0        7.0             4.0              0.0   \n",
       "317                3.0        3.0             0.0              1.0   \n",
       "796                2.0        9.0             4.0              2.0   \n",
       "425                2.0        1.0             0.0              0.0   \n",
       "991                3.0        7.0             0.0              0.0   \n",
       "\n",
       "     number_of_vehicles_involved  \n",
       "887                          0.0  \n",
       "317                          2.0  \n",
       "796                          0.0  \n",
       "425                          3.0  \n",
       "991                          0.0  \n",
       "\n",
       "[5 rows x 131 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sanity Check\n",
    "x_train_concat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Standardize the value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardizeData(data, scaler =None):\n",
    "    if scaler == None:\n",
    "        # Create Fit Scaler\n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(data)\n",
    "\n",
    "    # Transform data\n",
    "    data_scaled = scaler.transform(data)\n",
    "    data_scaled = pd.DataFrame(data_scaled,\n",
    "                                index = data.index,\n",
    "                                columns = data.columns)\n",
    "    \n",
    "    return data_scaled, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_clean, scaler = standardizeData(data = x_train_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 131)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_clean.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Change Label into Int format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_clean = y_train.map(dict(Y=1, N=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_clean.shape[0] == y_train_clean.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_clean = pd.concat([x_train_clean, y_train_clean], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.8 Label Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def le_fit(data_tobe_fitted: dict, le_path: str) -> LabelEncoder:\n",
    "    # Create le object\n",
    "    le_encoder = LabelEncoder()\n",
    "\n",
    "    # Fit le\n",
    "    le_encoder.fit(data_tobe_fitted)\n",
    "\n",
    "    # Save le object\n",
    "    util.pickle_dump(le_encoder, le_path)\n",
    "\n",
    "    # Return trained le\n",
    "    return le_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le_fit(config_data[\"label_categories\"], config_data[\"le_label_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Balancing Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balancing(data):\n",
    "    x_data = data.drop(columns = config_data['label'])\n",
    "    y_data = data[config_data['label']]\n",
    "\n",
    "    x_over, y_over = RandomOverSampler(random_state=42).fit_resample(x_data, y_data)\n",
    "    x_smote, y_smote = SMOTE(random_state=42).fit_resample(x_data, y_data)\n",
    "\n",
    "    train_set_smote = pd.concat([x_smote, y_smote], axis = 1)\n",
    "    train_set_over = pd.concat([x_over, y_over], axis = 1)\n",
    "\n",
    "    return x_smote, y_smote, x_over, y_over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_smote, y_smote, x_over, y_over = balancing(train_set_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.9 Handle Valid and Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling valid data using previous function\n",
    "## Splitting into num & cat, imputer num & cat, ohe & le, standarization\n",
    "\n",
    "def handlingData(set_data):\n",
    "    # Split data into x_data, y_data\n",
    "    x_data, y_data = splitxy(set_data)\n",
    "\n",
    "    # Split x_data into numerical and categorical\n",
    "    x_data_num, x_data_cat = splitNumCat(x_data)\n",
    "\n",
    "    # Encoding categorical data by separating it into OHE and Ordinal..\n",
    "    nominal = ['policy_state','policy_csl','policy_deductable','insured_sex','insured_hobbies','collision_type',\n",
    "                'authorities_contacted','incident_state','incident_city','property_damage','police_report_available',\n",
    "                'auto_make','auto_model']\n",
    "    ordinal = ['incident_type','witnesses','incident_severity','auto_year','umbrella_limit','bodily_injuries',\n",
    "            'number_of_vehicles_involved']\n",
    "\n",
    "    # Impute num data\n",
    "    x_data_num_imputed, imputer_num_ = imputerNum(data = x_data_num, imputer = imputer_num)\n",
    "\n",
    "    # Impute cat data\n",
    "    x_data_cat_imputed, imputer_cat_ = imputerCat(data = x_data_cat, imputer = imputer_cat)\n",
    "\n",
    "    x_data_cat_ohe, encoder_col_, encoder_ = OHEcat(x_data_cat_imputed, encoder_ohe_col,\n",
    "                                                            encoder_ohe)\n",
    "    x_data_cat_le, encoder_ = LEcat(x_data_cat_imputed, encoder_le)\n",
    "\n",
    "    x_data_cat_concat = pd.concat([x_data_cat_ohe, x_data_cat_le], axis=1)\n",
    "        \n",
    "    # Concatenate data numeric and categorical\n",
    "    x_data_concat = pd.concat([x_data_num_imputed, x_data_cat_concat], axis = 1)\n",
    "\n",
    "    # Standardize data using standarscaler\n",
    "    x_data_clean, scaler_ = standardizeData(x_data_concat, scaler)\n",
    "\n",
    "    y_data_clean = y_data.map(dict(Y=1, N=0))\n",
    "\n",
    "    train_set_clean = pd.concat([x_data_clean, y_data_clean], axis=1)\n",
    "\n",
    "    x_smote_set, y_smote_set, x_over_set, y_over_set = balancing(train_set_clean)\n",
    "\n",
    "    return x_data_clean, y_data_clean, x_smote_set, y_smote_set, x_over_set, y_over_set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.10 Create smote and oversampling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_11728\\1163776876.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.umbrella_limit = data.umbrella_limit.replace('-1000000','1000000')\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_11728\\1163776876.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[col] = data[col].replace('?', 'UNKNOWN')\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_11728\\1163776876.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.umbrella_limit = data.umbrella_limit.replace('-1000000','1000000')\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_11728\\1163776876.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[col] = data[col].replace('?', 'UNKNOWN')\n"
     ]
    }
   ],
   "source": [
    "x_valid_clean, y_valid_clean, \\\n",
    "x_valid_smote_clean, y_valid_smote_clean, \\\n",
    "x_valid_over_clean, y_valid_over_clean  = handlingData(valid_set)\n",
    "\n",
    "x_test_clean, y_test_clean, \\\n",
    "x_test_smote_clean, y_test_smote_clean, \\\n",
    "x_test_over_clean, y_test_over_clean = handlingData(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.76\n",
       "1    0.24\n",
       "Name: fraud_reported, dtype: float64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_clean.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. DUMP TRAINSET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_final = {\n",
    "    \"nonbalance\" : x_train_clean,\n",
    "    \"smote\" : x_smote,\n",
    "    \"oversampling\" : x_over\n",
    "}\n",
    "\n",
    "y_train_final = {\n",
    "    \"nonbalance\" : y_train_clean,\n",
    "    \"smote\" : y_smote,\n",
    "    \"oversampling\" : y_over\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.pickle_dump(x_train_final, config_data['train_set_clean'][0])\n",
    "util.pickle_dump(y_train_final, config_data['train_set_clean'][1])\n",
    "\n",
    "util.pickle_dump(x_valid_clean, config_data['valid_set_clean'][0])\n",
    "util.pickle_dump(y_valid_clean, config_data['valid_set_clean'][1])\n",
    "\n",
    "util.pickle_dump(x_test_clean, config_data['test_set_clean'][0])\n",
    "util.pickle_dump(y_test_clean, config_data['test_set_clean'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a90aeebcf29d64a654773811cc170cb25061cb2498f10ac689db374c7bf325de"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
